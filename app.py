# app.py
import streamlit as st
import pandas as pd
import pdfplumber
import re
import requests
from io import BytesIO
from datetime import datetime
from bs4 import BeautifulSoup

st.set_page_config(page_title="Auditor TR - Extra√ß√£o + CATMAT", layout="wide")
st.title("üõ°Ô∏è Auditor TR ‚Äî Extra√ß√£o fiel + Consulta CATMAT/CATSER")

# ---------- UTILIT√ÅRIOS ----------
def clean_number(value):
    if value is None: return 0.0
    s = str(value).strip()
    s = s.replace("R$", "").replace("\xa0", " ")
    # remove thousand separators and convert comma decimal to dot
    s = s.replace(".", "").replace(",", ".")
    s = re.sub(r"[^\d\.\-]", "", s)
    try:
        return float(s) if s != "" else 0.0
    except:
        return 0.0

def normalize_text(v):
    if v is None: return ""
    return str(v).strip()

# ---------- CONSULTA CATMAT / CATSER (API p√∫blica) ----------
# This function is the one you can swap for an internal API or other proxy.
@st.cache_data(show_spinner=False)
def consultar_item_cat(codigo):
    """
    Consulta nas APIs p√∫blicas de compras.dados.gov.br (materiais e servi√ßos).
    Retorna dict: {status_api, tipo, codigo, descricao, unidade, link}
    """
    code = re.sub(r"\D", "", str(codigo))
    if not code:
        return {"status_api": "Inv√°lido", "codigo": codigo, "descricao": "", "unidade": "-", "link": ""}

    # Tenta materiais
    try:
        url_mat = f"https://compras.dados.gov.br/materiais/v1/materiais.json?codigo={code}"
        r = requests.get(url_mat, timeout=6)
        if r.status_code == 200:
            j = r.json()
            mats = j.get("_embedded", {}).get("materiais", [])
            if mats:
                item = mats[0]
                unidade = item.get("unidade_medida") or item.get("unidade") or "-"
                return {
                    "status_api": "Encontrado-Mat",
                    "tipo": "Material",
                    "codigo": code,
                    "descricao": item.get("descricao", ""),
                    "unidade": unidade,
                    "link": f"https://catalogo.compras.gov.br/cnbs-web/busca?cod={code}"
                }
    except Exception:
        pass

    # Tenta servicos
    try:
        url_srv = f"https://compras.dados.gov.br/servicos/v1/servicos.json?codigo={code}"
        r = requests.get(url_srv, timeout=6)
        if r.status_code == 200:
            j = r.json()
            srvs = j.get("_embedded", {}).get("servicos", [])
            if srvs:
                item = srvs[0]
                unidade = item.get("unidade") or "UN"
                return {
                    "status_api": "Encontrado-Serv",
                    "tipo": "Servico",
                    "codigo": code,
                    "descricao": item.get("descricao", ""),
                    "unidade": unidade,
                    "link": f"https://catalogo.compras.gov.br/cnbs-web/busca?cod={code}"
                }
    except Exception:
        pass

    return {
        "status_api": "NaoEncontrado",
        "tipo": None,
        "codigo": code,
        "descricao": "",
        "unidade": "-",
        "link": f"https://catalogo.compras.gov.br/cnbs-web/busca?cod={code}"
    }

# ---------- EXTRA√á√ÉO / RECONSTRU√á√ÉO ----------
HEADER_KEYWORDS = [
    "item","descr","descricao","unidade","catmat","catser","qtd","quant","quantidade",
    "preco","pre√ßo","unit","unitario","total","s√£o paulo","sp","rio","recife","manaus","caet√©","caete"
]

def guess_header_index(rows):
    for i, row in enumerate(rows[:30]):
        if not any(cell for cell in row):
            continue
        row_text = " ".join([str(x).lower() for x in row if x])
        score = sum(1 for kw in HEADER_KEYWORDS if kw in row_text)
        if score >= 2:
            return i
    return None

def extract_tables_from_pdf(file_stream):
    """Extrai linhas tabulares usando pdfplumber e retorna lista de linhas (cada linha = lista de c√©lulas) e texto completo"""
    tabular_rows = []
    all_text = ""
    with pdfplumber.open(file_stream) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            all_text += text + "\n"
            # tenta extrair tabelas com estrat√©gia por linhas
            try:
                tables = page.extract_tables(table_settings={"vertical_strategy":"lines","horizontal_strategy":"lines"})
                if not tables:
                    tables = page.extract_tables()
            except Exception:
                tables = page.extract_tables()
            if tables:
                for t in tables:
                    for r in t:
                        row = [("" if c is None else c) for c in r]
                        if any(str(x).strip() for x in row):
                            tabular_rows.append(row)
    return tabular_rows, all_text

def rebuild_dataframe(tabular_rows, full_text):
    """Reconstr√≥i um DataFrame com colunas padr√£o, tentando manter a ordem do cabe√ßalho original"""
    if tabular_rows:
        header_idx = guess_header_index(tabular_rows)
        if header_idx is None:
            for i, r in enumerate(tabular_rows[:6]):
                if any(str(x).strip() for x in r):
                    header_idx = i
                    break
        if header_idx is not None and header_idx < len(tabular_rows)-1:
            header_row = [normalize_text(c) for c in tabular_rows[header_idx]]
            # create safe header names
            headers = []
            for i,h in enumerate(header_row):
                name = h if h else f"col{i}"
                headers.append(name)
            data_rows = tabular_rows[header_idx+1:]
            # pad/truncate rows
            processed = []
            for r in data_rows:
                row = [("" if c is None else c) for c in r]
                if len(row) < len(headers):
                    row += [""]*(len(headers)-len(row))
                processed.append(row[:len(headers)])
            df = pd.DataFrame(processed, columns=headers)
            df = df.applymap(lambda x: normalize_text(x) if isinstance(x, str) else x)
            # Try to normalize common column names
            # Map likely columns to standard names
            ren = {}
            for c in df.columns:
                lc = c.lower()
                if "descr" in lc or "espec" in lc:
                    ren[c] = "Descri√ß√£o"
                elif "cat" in lc or "cod" in lc:
                    ren[c] = "CATMAT"
                elif "unid" in lc or re.match(r"^u[np]$", lc):
                    ren[c] = "Unidade"
                elif "qtd" in lc or "quant" in lc:
                    ren[c] = "QTD"
                elif "s√£o paulo" in lc or lc.strip() in ("sp","s√£o paulo","sao paulo"):
                    ren[c] = "S√£o Paulo"
                elif "rio" in lc:
                    ren[c] = "Rio de Janeiro"
                elif "recife" in lc:
                    ren[c] = "Recife"
                elif "manaus" in lc:
                    ren[c] = "Manaus"
                elif "caet√©" in lc or "caete" in lc:
                    ren[c] = "Caet√©"
                elif "pre√ßo unit" in lc or "preco unit" in lc or ("unit" in lc and "total" not in lc):
                    ren[c] = "Pre√ßo Unit√°rio (R$)"
                elif "total" in lc and ("pre√ßo" in lc or "preco" in lc) or ("valor total" in lc):
                    ren[c] = "Pre√ßo Total (R$)"
            df = df.rename(columns=ren)
            # Ensure standard columns exist
            std_cols = ["Grupo","Item","Descri√ß√£o","Unidade","CATMAT","QTD","S√£o Paulo","Rio de Janeiro","Caet√©","Manaus","Recife","Pre√ßo Unit√°rio (R$)","Pre√ßo Total (R$)"]
            for c in std_cols:
                if c not in df.columns:
                    df[c] = ""
            # Try detecting group titles in surrounding text and filling Grupo:
            # Simple heuristic: if a row has all empty numeric columns and long text like "GRUPO X", mark it and propagate
            groups = []
            current_group = "SEM GRUPO"
            # try scan full_text for "GRUPO" headings and basic positions - fallback: use "SEM GRUPO"
            if "GRUPO" in full_text.upper():
                # find lines that contain GRUPO
                lines = full_text.splitlines()
                grp_positions = []
                for ln in lines:
                    if re.search(r"\bGRUPO\b", ln, re.IGNORECASE):
                        grp_positions.append(ln.strip())
                # choose first three if exist
                if grp_positions:
                    current_group = grp_positions[0]
            df["Grupo"] = current_group
            # Reorder to std_cols with Grupo first
            cols_order = ["Grupo","Item","Descri√ß√£o","Unidade","CATMAT","QTD","S√£o Paulo","Rio de Janeiro","Caet√©","Manaus","Recife","Pre√ßo Unit√°rio (R$)","Pre√ßo Total (R$)"]
            df = df[cols_order]
            # Convert numeric columns where possible
            for c in ["QTD","Pre√ßo Unit√°rio (R$)","Pre√ßo Total (R$)"]:
                if c in df.columns:
                    df[c] = df[c].apply(clean_number)
            return df
    # Fallback: try regex scan for codes in text
    rows = []
    codes = list(set(re.findall(r"\b\d{5,7}\b", full_text)))
    for c in codes:
        rows.append({"Grupo":"SEM GRUPO","Item":"","Descri√ß√£o":"","Unidade":"","CATMAT":c,"QTD":0,"S√£o Paulo":0,"Rio de Janeiro":0,"Caet√©":0,"Manaus":0,"Recife":0,"Pre√ßo Unit√°rio (R$)":0,"Pre√ßo Total (R$)":0})
    if rows:
        return pd.DataFrame(rows)
    return pd.DataFrame(columns=["Grupo","Item","Descri√ß√£o","Unidade","CATMAT","QTD","S√£o Paulo","Rio de Janeiro","Caet√©","Manaus","Recife","Pre√ßo Unit√°rio (R$)","Pre√ßo Total (R$)"])

# ---------- HTML GENERATION ----------
def generate_grouped_html(df):
    html_parts = []
    # if there's Grupo column, group by it
    if "Grupo" in df.columns:
        groups = df["Grupo"].fillna("SEM GRUPO").unique().tolist()
    else:
        groups = ["Todos os Itens"]
    for g in groups:
        sub = df[df["Grupo"].fillna("SEM GRUPO")==g].copy()
        html_parts.append(f"<h3>{g}</h3>")
        # use pandas to_html for table body
        html = sub.to_html(index=False, classes="table", escape=True)
        html_parts.append(html)
    return "\n".join(html_parts)

# ---------- STREAMLIT UI ----------
st.sidebar.header("Configura√ß√µes")
run_catmat_via_api = st.sidebar.selectbox("Consulta CATMAT via", ["API p√∫blica (compras.dados.gov.br)"], index=0)
uploaded = st.file_uploader("üìÇ Envie o TR (PDF)", type=["pdf"])

if not uploaded:
    st.info("Envie o PDF do Termo de Refer√™ncia para iniciar.")
    st.stop()

with st.spinner("Extraindo tabelas do PDF..."):
    rows, full_text = extract_tables_from_pdf(uploaded)

with st.spinner("Reconstruindo DataFrame..."):
    df = rebuild_dataframe(rows, full_text)

if df.empty:
    st.error("N√£o foi poss√≠vel extrair itens do PDF automaticamente. Se for um PDF escaneado (imagem), ative OCR externamente e reenvie.")
    st.stop()

# Show summary and table
st.markdown("### ‚úÖ Tabela extra√≠da (pr√©via)")
st.write(f"Linhas: {len(df)} ‚Äî Colunas: {', '.join(df.columns)}")
st.dataframe(df, use_container_width=True, height=360)

# Buttons: generate HTML, download Excel, run CATMAT sweep
c1, c2, c3 = st.columns([1,1,1])
with c1:
    if st.button("üìÑ Gerar/baixar HTML tabulado"):
        html_body = generate_grouped_html(df)
        full_html = f"""<!doctype html><html lang='pt-BR'><head><meta charset='utf-8'><title>TR - Tabela</title>
        <style>body{{font-family:Arial}}table{{border-collapse:collapse;width:100%}}th,td{{border:1px solid #bbb;padding:6px}}th{{background:#eee}}</style></head><body>
        <h2>Termo de Refer√™ncia ‚Äî Itens (extra√≠do)</h2>{html_body}</body></html>"""
        st.download_button("‚¨áÔ∏è Baixar HTML", data=full_html.encode("utf-8"), file_name="tabela_final.html", mime="text/html")

with c2:
    if st.button("‚¨áÔ∏è Gerar e baixar Excel consolidado"):
        with BytesIO() as buf:
            with pd.ExcelWriter(buf, engine="openpyxl") as writer:
                df.to_excel(writer, sheet_name="Itens", index=False)
            buf.seek(0)
            st.download_button("‚¨áÔ∏è Baixar Excel", data=buf.getvalue(), file_name="tabela_final.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

with c3:
    if st.button("üîé Consultar CATMAT para c√≥digos detectados"):
        st.info("Executando varredura ‚Äî aguarde (cada consulta usa a API p√∫blica).")
        # find code column: try CATMAT, then any column with numeric codes
        code_col = None
        for c in df.columns:
            if c.lower() in ("catmat","catser","codigo","cod","c√≥d"):
                code_col = c; break
        if not code_col:
            # try search in column values
            for c in df.columns:
                sample = df[c].astype(str).head(30).tolist()
                if any(re.search(r"\b\d{5,7}\b", s) for s in sample):
                    code_col = c; break
        if not code_col:
            st.error("N√£o localizei uma coluna de c√≥digos (CATMAT/CATSER). Renomeie a coluna ou informe manualmente.")
        else:
            st.info(f"Usando coluna: {code_col}")
            codes = df[code_col].astype(str).fillna("").unique().tolist()
            codes = [re.sub(r"\D","",c) for c in codes if re.search(r"\d{5,7}", str(c))]
            total = len(codes)
            progress = st.progress(0)
            results = []
            for i, code in enumerate(codes):
                progress.progress((i+1)/max(1,total))
                res = consultar_item_cat(code)
                results.append(res)
            df_cat = pd.DataFrame(results)
            # join results back to df on CATMAT where possible
            # Provide combined Excel with two sheets
            with BytesIO() as buf:
                with pd.ExcelWriter(buf, engine="openpyxl") as writer:
                    df.to_excel(writer, sheet_name="Itens", index=False)
                    df_cat.to_excel(writer, sheet_name="CATMAT", index=False)
                buf.seek(0)
                st.download_button("‚¨áÔ∏è Baixar Excel (Itens + CATMAT)", data=buf.getvalue(), file_name="auditoria_com_catmat.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
            st.dataframe(df_cat, use_container_width=True, height=300)

# Additional checks (math and simple flags)
st.markdown("### ‚ÑπÔ∏è Verifica√ß√µes r√°pidas")
if "QTD" in df.columns and "Pre√ßo Unit√°rio (R$)" in df.columns and "Pre√ßo Total (R$)" in df.columns:
    df_check = df.copy()
    df_check["Total Calculado"] = df_check["QTD"] * df_check["Pre√ßo Unit√°rio (R$)"]
    df_check["Diff"] = (df_check["Total Calculado"] - df_check["Pre√ßo Total (R$)"]).abs()
    df_check["Status Math"] = df_check["Diff"].apply(lambda d: "OK" if d <= 0.1 else "DIVERGENTE")
    problemas = df_check[df_check["Status Math"]!="OK"]
    st.write(f"Linhas com diverg√™ncia matem√°tica: {len(problemas)}")
    if not problemas.empty:
        st.dataframe(problemas[["Item","CATMAT","QTD","Pre√ßo Unit√°rio (R$)","Pre√ßo Total (R$)","Total Calculado","Diff"]], height=260)
else:
    st.info("Colunas QTD e Pre√ßo Unit√°rio/Total n√£o detectadas juntas ‚Äî verifica√ß√£o matem√°tica desativada.")
